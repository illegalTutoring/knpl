{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "from logger import utils\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pydub import AudioSegment\n",
    "from logger.utils import traverse_dir\n",
    "\n",
    "# Cuda setting\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# configure loading\n",
    "args = utils.load_config('./configs/sins.yaml')\n",
    "\n",
    "# set path\n",
    "MP4_DATA_PATH   = 'preprocess/mp4'\n",
    "ORIGINAL_PATH   = 'preprocess/original/'\n",
    "DEMUCS_PATH     = 'preprocess/demucs/'\n",
    "NORM_PATH       = 'preprocess/norm/'\n",
    "TEMP_LOG_PATH   = 'temp_ffmpeg_log.txt'  # ffmpeg의 무음 감지 로그의 임시 저장 위치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.데이터 전처리\n",
    "***\n",
    "1. 난 전처리가 필요없다. 배경음 제거가 완벽하고, 모든 데이터들도 특정 길이로 잘 잘려져있다.\n",
    "    - 데이터를 전부 data/train/audio/안에 다 집어 넣고\n",
    "        ```\n",
    "        # training dataset\n",
    "        data/train/audio/aaa.wav\n",
    "        data/train/audio/bbb.wav\n",
    "        ...\n",
    "        ```\n",
    "    - 1.6 validation 분리단계로 점프\n",
    "***\n",
    "2. 난 거의 다 되어 있지만 데이터가 너무 길다. 특정 길이로 자르고 싶다.\n",
    "    - 데이터를 전부 preprocess/split 안에 다 집어 넣고\n",
    "        ```\n",
    "        # training dataset\n",
    "        preprocess/norm/aaa.wav\n",
    "        preprocess/norm/bbb.wav\n",
    "        ...\n",
    "        ```\n",
    "    - 1.4 split 단계로 점프\n",
    "***\n",
    "3. 난 배경음도 제거해야되고 데이터도 길다. 거의 날 것의 상태다.\n",
    "    - 데이터를 전부 preprocess/original 안에 다 집어넣고\n",
    "        ```\n",
    "        # training dataset\n",
    "        preprocess/original/aaa.wav\n",
    "        preprocess/original/bbb.wav\n",
    "        ...\n",
    "        ```\n",
    "    - 1.2부터 demucs 단계로 점프\n",
    "***\n",
    "4. 난 아무것도 안되어 있고, 심지어 mp4파일이다.\n",
    "    - 데이터들 전부 preprocess/mp4에 다 집어넣고\n",
    "        ```\n",
    "        # training dataset\n",
    "        preprocess/mp4/aaa.mp4\n",
    "        preprocess/mp4/bbb.mp4\n",
    "        ...\n",
    "        ```\n",
    "    - 1.1부터 차례대로 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1데이터가 mp4인 경우 (wav만 있는 경우에는 패스)\n",
    "preprocess/mp4 안에 있는 mp4파일을 wav로 변경 해서 preprocess/original 에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "def mp4_to_wav(input_dir : MP4_DATA_PATH, input_file: \"bb.mp4\", output_dir: ORIGINAL_PATH):\n",
    "    \"\"\"mp4파일을 wav형식으로 변환합니다.\n",
    "    Args:\n",
    "        input_dir (str) : 입력 mp4파일의 path\n",
    "        input_file (str) : 입력 mp4파일의 이름\n",
    "        output_dir (str) : 출력 wav파일의 path\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(input_file)[1][1:]\n",
    "\n",
    "    if ext != \"mp4\":\n",
    "        return \n",
    "    else :\n",
    "        track = AudioSegment.from_file(os.path.join(input_dir,input_file),  format= 'mp4')\n",
    "        track = track.set_frame_rate(44100)\n",
    "        track.export(os.path.join(output_dir, input_file[:-4]+\".wav\"), format='wav')\n",
    "\n",
    "\n",
    "filelist =  traverse_dir(\n",
    "    MP4_DATA_PATH,\n",
    "    extension='mp4',\n",
    "    is_pure=True,\n",
    "    is_sort=True,\n",
    "    is_ext=True)\n",
    "\n",
    "for file in tqdm(filelist):\n",
    "    mp4_to_wav(MP4_DATA_PATH, file, ORIGINAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 무음제거 (Demucs)\n",
    "preprocess/original에 있는 wav파일들의 음악소리를 제거하고 목소리만 추출해서 preprocess/demucs 에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 44100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "목소리 추출 중...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.99s/it]\n"
     ]
    }
   ],
   "source": [
    "from sep_wav import demucs\n",
    "\n",
    "demucs(ORIGINAL_PATH, DEMUCS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 normalize\n",
    "preprocess/demucs에 있는 배경음이 제거된 데이터들을 노멀라이즈 (sample rate값을 바꿀 수 있음) 해서 preprocess/norm에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "노멀라이징 작업 중...: 100%|█████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.35s/it]\n"
     ]
    }
   ],
   "source": [
    "from sep_wav import audio_norm\n",
    "\n",
    "for filepath in tqdm(glob(DEMUCS_PATH+\"*.wav\"), desc=\"노멀라이징 작업 중...\"):\n",
    "    filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    out_filepath = os.path.join(NORM_PATH, filename) + \".wav\"\n",
    "    audio_norm(filepath, out_filepath, sample_rate = 44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 split\n",
    "preprocess/norm에 있는 노말라이즈된 데이터들을 15초 길이로 잘라서 data/train/audio에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "음원 자르는 중...:   0%|                                                                         | 0/3 [00:00<?, ?it/s]C:\\Users\\csni_\\AppData\\Local\\Temp\\ipykernel_12884\\1509468694.py:4: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  duration = librosa.get_duration(filename=filepath)\n",
      "음원 자르는 중...: 100%|█████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "for filepath in tqdm(glob(NORM_PATH+\"*.wav\"), desc=\"음원 자르는 중...\"):\n",
    "    duration = librosa.get_duration(filename=filepath)\n",
    "    max_last_seg_duration = 0\n",
    "    sep_duration_final = 15\n",
    "    sep_duration = 15\n",
    "\n",
    "    while sep_duration > 4:\n",
    "        last_seg_duration = duration % sep_duration\n",
    "        if max_last_seg_duration < last_seg_duration:\n",
    "            max_last_seg_duration = last_seg_duration\n",
    "            sep_duration_final = sep_duration\n",
    "        sep_duration -= 1\n",
    "\n",
    "    filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    out_filepath = os.path.join(args.data.train_path,\"audio\", f\"{filename}-%04d.wav\")\n",
    "    subprocess.run(f'ffmpeg -i \"{filepath}\" -f segment -segment_time {sep_duration_final} \"{out_filepath}\" -y', capture_output=True, shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 무음 제거\n",
    "data/train/audio에 있는 잘라진 음원들 중에 무음인 파일들을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "무음 제거 중...: 100%|███████████████████████████████████████████████████████████████| 130/130 [00:06<00:00, 19.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from sep_wav import get_ffmpeg_args\n",
    "import subprocess\n",
    "\n",
    "for filepath in tqdm(glob(args.data.train_path+\"/audio/*.wav\"), desc=\"무음 제거 중...\"):\n",
    "    if os.path.exists(TEMP_LOG_PATH):\n",
    "        os.remove(TEMP_LOG_PATH)\n",
    "\n",
    "    ffmpeg_arg = get_ffmpeg_args(filepath)\n",
    "    subprocess.run(ffmpeg_arg, capture_output=True, shell=True)\n",
    "\n",
    "    start = None\n",
    "    end = None\n",
    "\n",
    "    with open(TEMP_LOG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            if \"lavfi.silence_start\" in line:\n",
    "                start = float(line.split(\"=\")[1])\n",
    "            if \"lavfi.silence_end\" in line:\n",
    "                end = float(line.split(\"=\")[1])\n",
    "\n",
    "    if start != None:\n",
    "        if start == 0 and end == None:\n",
    "            os.remove(filepath)\n",
    "            \n",
    "if os.path.exists(TEMP_LOG_PATH):\n",
    "        os.remove(TEMP_LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 학습 데이터 중, 일부를 validaion용으로 자동으로 보내준다.\n",
    "- data/train/audio에 있는 데이터 중 일정 비율만큼 알아서 data/val/audio로 이동시켜준다\n",
    "    - 계산식은 다음과 같다 `max(2, min(10, 전체 데이터 * 0.01))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1000.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from draw import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리 (학습에 쓰기 위한)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Encoder Model] HuBERT Soft\n",
      " [Loading] pretrain/hubert/hubert-soft-0d54a1f4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csni_\\anaconda3\\envs\\ddsp\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess the audio clips in : data/train\\audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/127 [00:00<?, ?it/s]C:\\Users\\csni_\\anaconda3\\envs\\ddsp\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:685: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 127/127 [00:13<00:00,  9.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess the audio clips in : data/val\\audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 13.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from preprocess import preprocess\n",
    "from ddsp.vocoder import F0_Extractor, Volume_Extractor, Units_Encoder\n",
    "from diffusion.vocoder import Vocoder\n",
    "\n",
    "# get data\n",
    "sample_rate = args.data.sampling_rate\n",
    "hop_size = args.data.block_size\n",
    "\n",
    "# initialize f0 extractor\n",
    "f0_extractor = F0_Extractor(\n",
    "                    args.data.f0_extractor, \n",
    "                    args.data.sampling_rate, \n",
    "                    args.data.block_size, \n",
    "                    args.data.f0_min, \n",
    "                    args.data.f0_max)\n",
    "\n",
    "# initialize volume extractor\n",
    "volume_extractor = Volume_Extractor(args.data.block_size)\n",
    "\n",
    "# initialize mel extractor\n",
    "mel_extractor = None\n",
    "if args.model.type == 'Diffusion':\n",
    "    mel_extractor = Vocoder(args.vocoder.type, args.vocoder.ckpt, device = device)\n",
    "    if mel_extractor.vocoder_sample_rate != sample_rate or mel_extractor.vocoder_hop_size != hop_size:\n",
    "        mel_extractor = None\n",
    "        print('Unmatch vocoder parameters, mel extraction is ignored!')\n",
    "\n",
    "# initialize units encoder\n",
    "if args.data.encoder == 'cnhubertsoftfish':\n",
    "    cnhubertsoft_gate = args.data.cnhubertsoft_gate\n",
    "else:\n",
    "    cnhubertsoft_gate = 10             \n",
    "units_encoder = Units_Encoder(\n",
    "                    args.data.encoder, \n",
    "                    args.data.encoder_ckpt, \n",
    "                    args.data.encoder_sample_rate, \n",
    "                    args.data.encoder_hop_size, \n",
    "                    device = device)    \n",
    "\n",
    "# preprocess training set\n",
    "preprocess(args.data.train_path, f0_extractor, volume_extractor, mel_extractor, units_encoder, sample_rate, hop_size, device = device)\n",
    "\n",
    "# preprocess validation set\n",
    "preprocess(args.data.valid_path, f0_extractor, volume_extractor, mel_extractor, units_encoder, sample_rate, hop_size, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\csni_\\anaconda3\\envs\\ddsp\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csni_\\anaconda3\\envs\\ddsp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-01 01:18:30 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >    exp: exp/sins-test\n",
      " [DDSP Model] Sinusoids Additive Synthesiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csni_\\anaconda3\\envs\\ddsp\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load all the data from : data/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/127 [00:00<?, ?it/s]C:\\Users\\csni_\\Desktop\\DDSP-SVC-KOR-master\\data_loaders.py:125: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  duration = librosa.get_duration(filename = path_audio, sr = self.sample_rate)\n",
      "  1%|▋                                                                                 | 1/127 [00:01<03:02,  1.45s/it]C:\\Users\\csni_\\Desktop\\DDSP-SVC-KOR-master\\data_loaders.py:125: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  duration = librosa.get_duration(filename = path_audio, sr = self.sample_rate)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 127/127 [00:05<00:00, 24.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load all the data from : data/val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 37.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- model size ---\n",
      "model: 3,375,360\n",
      "======= start training =======\n",
      "epoch: 1 |   3/  6 | exp/sins-test | batch/s: 1.28 | loss: 4.341 | time: 0:00:07.9 | step: 10\n",
      "epoch: 3 |   1/  6 | exp/sins-test | batch/s: 4.85 | loss: 3.689 | time: 0:00:10.0 | step: 20\n",
      "epoch: 4 |   5/  6 | exp/sins-test | batch/s: 4.70 | loss: 2.859 | time: 0:00:12.0 | step: 30\n",
      "epoch: 6 |   3/  6 | exp/sins-test | batch/s: 4.69 | loss: 2.914 | time: 0:00:14.2 | step: 40\n",
      "epoch: 8 |   1/  6 | exp/sins-test | batch/s: 4.88 | loss: 2.645 | time: 0:00:16.3 | step: 50\n",
      "epoch: 9 |   5/  6 | exp/sins-test | batch/s: 4.52 | loss: 2.465 | time: 0:00:18.4 | step: 60\n",
      "epoch: 11 |   3/  6 | exp/sins-test | batch/s: 4.63 | loss: 2.038 | time: 0:00:20.6 | step: 70\n",
      "epoch: 13 |   1/  6 | exp/sins-test | batch/s: 4.77 | loss: 2.367 | time: 0:00:22.7 | step: 80\n",
      "epoch: 14 |   5/  6 | exp/sins-test | batch/s: 4.70 | loss: 1.517 | time: 0:00:24.8 | step: 90\n",
      "epoch: 16 |   3/  6 | exp/sins-test | batch/s: 4.72 | loss: 1.760 | time: 0:00:27.0 | step: 100\n",
      "epoch: 18 |   1/  6 | exp/sins-test | batch/s: 4.72 | loss: 1.736 | time: 0:00:29.1 | step: 110\n",
      "epoch: 19 |   5/  6 | exp/sins-test | batch/s: 4.62 | loss: 1.676 | time: 0:00:31.2 | step: 120\n",
      "epoch: 21 |   3/  6 | exp/sins-test | batch/s: 4.70 | loss: 1.626 | time: 0:00:33.4 | step: 130\n",
      "epoch: 23 |   1/  6 | exp/sins-test | batch/s: 4.70 | loss: 1.883 | time: 0:00:35.5 | step: 140\n",
      "epoch: 24 |   5/  6 | exp/sins-test | batch/s: 4.59 | loss: 2.015 | time: 0:00:37.6 | step: 150\n",
      "epoch: 26 |   3/  6 | exp/sins-test | batch/s: 4.69 | loss: 1.750 | time: 0:00:39.8 | step: 160\n",
      "epoch: 28 |   1/  6 | exp/sins-test | batch/s: 4.83 | loss: 1.701 | time: 0:00:41.9 | step: 170\n",
      "epoch: 29 |   5/  6 | exp/sins-test | batch/s: 4.59 | loss: 1.526 | time: 0:00:44.0 | step: 180\n",
      "epoch: 31 |   3/  6 | exp/sins-test | batch/s: 4.64 | loss: 1.738 | time: 0:00:46.2 | step: 190\n",
      "epoch: 33 |   1/  6 | exp/sins-test | batch/s: 4.79 | loss: 1.674 | time: 0:00:48.3 | step: 200\n",
      "epoch: 34 |   5/  6 | exp/sins-test | batch/s: 4.61 | loss: 1.782 | time: 0:00:50.4 | step: 210\n",
      "epoch: 36 |   3/  6 | exp/sins-test | batch/s: 4.66 | loss: 1.521 | time: 0:00:52.6 | step: 220\n",
      "epoch: 38 |   1/  6 | exp/sins-test | batch/s: 4.80 | loss: 1.700 | time: 0:00:54.7 | step: 230\n",
      "epoch: 39 |   5/  6 | exp/sins-test | batch/s: 4.73 | loss: 1.550 | time: 0:00:56.7 | step: 240\n",
      "epoch: 41 |   3/  6 | exp/sins-test | batch/s: 4.76 | loss: 1.483 | time: 0:00:58.9 | step: 250\n",
      "epoch: 43 |   1/  6 | exp/sins-test | batch/s: 4.93 | loss: 1.525 | time: 0:01:01.0 | step: 260\n",
      "epoch: 44 |   5/  6 | exp/sins-test | batch/s: 4.69 | loss: 1.485 | time: 0:01:03.0 | step: 270\n",
      "epoch: 46 |   3/  6 | exp/sins-test | batch/s: 4.72 | loss: 1.454 | time: 0:01:05.2 | step: 280\n",
      "epoch: 48 |   1/  6 | exp/sins-test | batch/s: 4.89 | loss: 1.529 | time: 0:01:07.3 | step: 290\n",
      "epoch: 49 |   5/  6 | exp/sins-test | batch/s: 4.70 | loss: 1.344 | time: 0:01:09.3 | step: 300\n",
      "epoch: 51 |   3/  6 | exp/sins-test | batch/s: 4.81 | loss: 1.390 | time: 0:01:11.5 | step: 310\n",
      "epoch: 53 |   1/  6 | exp/sins-test | batch/s: 4.97 | loss: 1.351 | time: 0:01:13.5 | step: 320\n",
      "epoch: 54 |   5/  6 | exp/sins-test | batch/s: 4.73 | loss: 1.385 | time: 0:01:15.5 | step: 330\n",
      "epoch: 56 |   3/  6 | exp/sins-test | batch/s: 4.81 | loss: 1.475 | time: 0:01:17.7 | step: 340\n",
      "epoch: 58 |   1/  6 | exp/sins-test | batch/s: 4.94 | loss: 1.331 | time: 0:01:19.7 | step: 350\n",
      "epoch: 59 |   5/  6 | exp/sins-test | batch/s: 4.76 | loss: 1.569 | time: 0:01:21.7 | step: 360\n",
      "epoch: 61 |   3/  6 | exp/sins-test | batch/s: 4.74 | loss: 1.271 | time: 0:01:23.9 | step: 370\n",
      "epoch: 63 |   1/  6 | exp/sins-test | batch/s: 4.90 | loss: 1.397 | time: 0:01:25.9 | step: 380\n",
      "epoch: 64 |   5/  6 | exp/sins-test | batch/s: 4.70 | loss: 1.236 | time: 0:01:28.0 | step: 390\n",
      "epoch: 66 |   3/  6 | exp/sins-test | batch/s: 4.77 | loss: 1.438 | time: 0:01:30.2 | step: 400\n",
      "epoch: 68 |   1/  6 | exp/sins-test | batch/s: 4.73 | loss: 1.378 | time: 0:01:32.3 | step: 410\n"
     ]
    }
   ],
   "source": [
    "from train import ddsp_train\n",
    "\n",
    "ddsp_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 결과물 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [DDSP Model] Sinusoids Additive Synthesiser\n",
      " [Loading] exp/sins-test/model_36000.pt\n",
      "MD5: 76efd3ee7d434a6fde37ce6c8a10fa23\n",
      "Pitch extractor type: crepe\n",
      "Extracting the pitch curve of the input audio...\n",
      "Extracting the volume envelope of the input audio...\n",
      " [Encoder Model] HuBERT Soft\n",
      " [Loading] pretrain/hubert/hubert-soft-0d54a1f4.pt\n",
      "Enhancer type: nsf-hifigan\n",
      "| Load HifiGAN:  pretrain/nsf_hifigan/model\n",
      "Removing weight norm...\n",
      "Speaker ID: 1\n",
      "Cut the input audio into 7 slices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from main import inference\n",
    "# configure setting\n",
    "configures = {\n",
    "    'model_path'            :   'exp/sins-test/model_36000.pt', # 추론에 사용하고자 하는 모델, 바로위에서 학습한 모델을 가져오면댐\n",
    "    'input'                 :   'data/test/audio/other.wav', # 추론하고자 하는 노래파일의 위치 - 님들이 바꿔야댐 \n",
    "    'output'                :   'data/test/audio/other_36000.wav',  # 결과물 파일의 위치\n",
    "    'device'                :   'cuda',\n",
    "    'spk_id'                :   '1', \n",
    "    'spk_mix_dict'          :   'None', \n",
    "    'key'                   :   '0', \n",
    "    'enhance'               :   'true' , \n",
    "    'pitch_extractor'       :   'crepe' ,\n",
    "    'f0_min'                :   '50' ,\n",
    "    'f0_max'                :   '1100',\n",
    "    'threhold'              :   '-60',\n",
    "    'enhancer_adaptive_key' :   '0'\n",
    "}\n",
    "cmd = SimpleNamespace(**configures)\n",
    "\n",
    "inference(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to operator (556115758.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    tensorboard --logdir=exp\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to operator\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir=exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8a643f5fe528358e1cfac3836870fd104c9c787e6f994a831162d9d1f5f0281"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
